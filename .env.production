# ArquimedesAI Production Configuration (v1.3.1)
# Optimized for maximum quality with all features enabled
# Copy this file to .env for production deployment

# Paths
ARQ_DATA_DIR=./data
ARQ_STORAGE_DIR=./storage

# Vector Database
ARQ_DB=qdrant
# ARQ_QDRANT_URL=http://localhost:6333  # Uncomment for remote Qdrant
ARQ_QDRANT_PATH=./storage/qdrant  # Local persistence

# Qdrant HNSW Optimization (v1.3.1)
# Production settings: High Precision + Low Memory strategy for quality on 8-16GB RAM
ARQ_QDRANT_HNSW_M=32              # Graph connectivity (higher=better accuracy)
ARQ_QDRANT_HNSW_EF_CONSTRUCT=256  # Build quality (higher=better index)
ARQ_QDRANT_ON_DISK=true           # Store on disk (lower memory, good for modest hardware)

# Embeddings
ARQ_EMBED_MODEL=BAAI/bge-m3

# Retrieval - Quality Funnel Strategy
# Fetch 50 candidates → Rerank → Return top 3 best results
ARQ_TOP_K=8           # Base retrieval (fallback when reranking disabled)
ARQ_FETCH_K=50        # Fetch many candidates for reranker to choose from

# Hybrid Retrieval (BM25 + Dense) - ENABLED
ARQ_HYBRID=true
ARQ_BM25_WEIGHT=0.4   # Keyword matching weight
ARQ_DENSE_WEIGHT=0.6  # Semantic similarity weight

# Cross-Encoder Reranking - ENABLED for best quality
ARQ_RERANK_ENABLED=true
ARQ_RERANK_MODEL=BAAI/bge-reranker-v2-m3
ARQ_RERANK_TOP_N=3    # Final top results after reranking

# Docling Configuration (v1.3) - ENABLED for best quality
ARQ_DOCLING_OCR=true         # Enable OCR for scanned PDFs/images
ARQ_DOCLING_TABLE_MODE=accurate  # Use accurate TableFormer model for best table extraction

# Ollama LLM
ARQ_OLLAMA_BASE=http://localhost:11434
ARQ_OLLAMA_MODEL=gemma3:latest
ARQ_OLLAMA_TEMPERATURE=0.3

# Conversational Memory (v1.4)
ARQ_ENABLE_CONVERSATION_MEMORY=true
ARQ_MAX_HISTORY_MESSAGES=20

# Structured Citations (v1.4)
ARQ_USE_STRUCTURED_CITATIONS=true
ARQ_CITATION_STYLE=quoted  # Options: quoted (with verbatim quotes) or id (source IDs only)

# Discord Bot
ARQ_DISCORD_TOKEN=your_discord_bot_token_here

# Performance Notes (v1.3.1):
# - Docling HybridChunker: Structure-aware, tokenization-optimized chunking
# - HNSW optimization: Better retrieval accuracy with lower memory (on_disk=true)
# - Reranking adds ~100-300ms latency but significantly improves relevance
# - Total model downloads on first run: ~3GB (BGE-M3 + reranker + Ollama)
# - Requires manual Ollama setup: ollama pull gemma3:latest

# Phase 1 (v1.4) Notes:
# - Conversational memory: In-session conversation history for follow-up questions
# - Structured citations: Pydantic schemas for reliable citation extraction
# - Use --conversational flag in CLI: python cli.py chat --conversational
